import time
import html
import json
import os
from typing import List, Dict, Optional
import logging
from datetime import datetime, timedelta
from curl_cffi import requests
from bs4 import BeautifulSoup
from dateutil import parser as date_parser
from dateutil import tz
from app.core import config
from app.core import database
from app.services import telegram_bot
from app.services import ai_engine

logger = config.logger

SCHEDULE_JSON_URL = "https://nfs.faireconomy.media/ff_calendar_thisweek.json"
CACHE_FILE = "data/ff_schedule.json"
CACHE_TTL = 3600  # 60 minutes

class EconomicCalendarService:
    def __init__(self):
        self.base_url = "https://www.forexfactory.com/calendar"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept-Language": "en-US,en;q=0.5"
        }
        # Ensure data dir exists
        if not os.path.exists("data"):
            os.makedirs("data")

    def fetch_schedule_json(self) -> List[Dict]:
        """
        Láº¥y lá»‹ch sá»± kiá»‡n tá»« JSON API (High Performance).
        Cache káº¿t quáº£ Ä‘á»ƒ giáº£m táº£i.
        """
        try:
            # 1. Check Cache
            if os.path.exists(CACHE_FILE):
                mod_time = os.path.getmtime(CACHE_FILE)
                if time.time() - mod_time < CACHE_TTL:
                    with open(CACHE_FILE, 'r') as f:
                        logger.info("ğŸ“¦ Loading schedule from cache...")
                        return json.load(f)
            
            # 2. Fetch from URL
            logger.info(f"ğŸŒ Fetching Schedule JSON: {SCHEDULE_JSON_URL}")
            response = requests.get(SCHEDULE_JSON_URL, impersonate="chrome120", timeout=30)
            if response.status_code == 200:
                data = response.json()
                
                # Filter useful events (High/Medium Impact only?)
                # user requests High/Medium usually, let's stick to user request's implicit logic or filter all?
                # Sticking to fetch all, filtering is done later or by impact check.
                # Actually, JSON usually contains everything.
                
                # Save to Cache
                with open(CACHE_FILE, 'w') as f:
                    json.dump(data, f)
                return data
            else:
                logger.error(f"âŒ Failed to fetch schedule JSON. Status: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"âŒ Error fetching schedule JSON: {e}")
            return []

    def sync_schedule_to_db(self):
        """
        Äá»“ng bá»™ lá»‹ch tá»« JSON vÃ o DB.
        LÆ°u Ã½: JSON nÃ y thÆ°á»ng khÃ´ng cÃ³ Actual ngay láº­p tá»©c hoáº·c khÃ´ng cÃ³ Actual.
        NÃªn chá»‰ dÃ¹ng Ä‘á»ƒ lÃªn lá»‹ch (Insert pending).
        """
        events = self.fetch_schedule_json()
        count = 0
        for item in events:
            # Map JSON fields to DB fields
            # Example JSON item: 
            # {"title": "French Flash Services PMI", "country": "EUR", "date": "2024-01-24T08:15:00-05:00", "impact": "Medium", "forecast": "49.0", "previous": "48.8"}
            
            # Create a unique ID if not present provided by JSON?
            # FF JSON usually doesn't have ID. We might need to generate one or URL has it?
            # Actually URL-based fetching had IDs.
            # Limitation of public JSON: No IDs.
            # Strategy: Use Title + Date + Currency as ID?
            
            # WAIT: The prompt says "Táº£i JSON... Parse vÃ  lÆ°u vÃ o DB".
            # If JSON lacks IDs matching the HTML `data-event-id`, we have a problem mapping Real-time HTML results (which have IDs) to these records.
            # Let's check if the generic JSON has IDs. Usually `ff_calendar_thisweek.json` is a scraped version or official?
            # "https://nfs.faireconomy.media/ff_calendar_thisweek.json" is the Official FF widget data source.
            # It usually looks like:
            # [{"title":"...","country":"USD","date":"...","impact":"High","forecast":"","previous":""}, ...]
            # IT DOES NOT HAVE IDs.
            
            # CRITICAL: We need to map HTML results (which we find by text? or just by scraping matching row?).
            # In HTML scraping: `event_title = row.find...`
            # We can map by (Title, Currency, Date).
            
            # Let's generate a synthetic ID: {date}_{currency}_{title_slug}
            
            try:
                title = item.get('title', 'Unknown')
                currency = item.get('country', 'USD') # JSON keys might differ, assuming 'country' map to currency or we map Country->Currency
                # Actually FF JSON uses 'country' (e.g. USD, EUR, GBP) which is effectively Currency for major pairs.
                
                impact = item.get('impact', 'Low')
                date_str = item.get('date') # ISO format with offset? e.g. "2024-01-24T08:15:00-05:00"
                
                # Parse date
                dt = date_parser.parse(date_str)
                # Convert to UTC
                dt_utc = dt.astimezone(tz.UTC)
                timestamp_iso = dt_utc.strftime('%Y-%m-%d %H:%M:%S')
                
                # Generate Synthetic ID
                # To match HTML later, HTML scraping needs to generate same ID or we update by (Title + Date)?
                # Updating by key constraints is cleaner.
                # Let's iterate and use Title+Timestamp as strict key.
                
                # Note: HTML Scraper gets `event_id` from `data-event-id`.
                # If we switch to JSON for Schedule, we lack `data-event-id`.
                # BUT, the User requirement says: "DÃ¹ng BeautifulSoup Ä‘á»ƒ tÃ¬m Ä‘Ãºng dÃ²ng sá»± kiá»‡n (theo data-event-id)".
                # This implies the HTML scraping relies on `data-event-id`.
                # If Schedule (JSON) doesn't provide it, we can't link them easily unless we obtain IDs from JSON.
                # If the URL `nfs.faireconomy.media` doesn't provide IDs, then "Use JSON for Schedule" and "Use HTML (by ID) for Result" is contradictory 
                # UNLESS we fetch Schedule via HTML too initially?
                # User said: "Lá»‹ch trÃ¬nh (Schedule): Láº¥y tá»« JSON API... Káº¿t quáº£: DÃ¹ng curl_cffi... quÃ©t HTML".
                # This implies JSON is the SOURCE of truth for Schedule.
                # If JSON has no ID, we cannot use `data-event-id` from HTML to update the specific row from JSON.
                # We must match by Title/Time.
                
                # Alternative: The user might believe JSON has IDs. or I should construct ID compatible.
                # Let's assume we use a deterministic ID based on Title+Time.
                # `id = f"{timestamp_iso}_{title}"`
                # When scraping HTML, we also construct this ID or we search DB by Title+Time?
                # Searching DB by Title+Time is safer.
                
                # Refined Plan:
                # 1. Schedule (JSON): upsert events with ID = "{timestamp}_{currency}_{title}"
                # 2. Result (HTML): Find row, extract Title, Time, Currency -> recreate ID -> Update Actual.
                #    OR: Just strict match Title and approximate Time.
                
                id_str = f"{timestamp_iso}_{currency}_{title}".replace(" ", "_").replace("/", "").replace(":", "")
                
                event_dict = {
                    "id": id_str,
                    "event": title, # legacy key
                    "title": title,
                    "currency": currency,
                    "impact": impact,
                    "timestamp": timestamp_iso,
                    "forecast": item.get('forecast', ''),
                    "previous": item.get('previous', ''),
                    "actual": "" # No actual in schedule sync
                }
                
                # Upsert
                database.upsert_economic_event(event_dict)
                count += 1
                
            except Exception as e:
                logger.warning(f"Skipping JSON item: {e}")
                continue
                
        logger.info(f"âœ… Synced {count} events from JSON Schedule.")

    def fetch_realtime_results_html(self):
        """
        QuÃ©t HTML Ä‘á»ƒ láº¥y káº¿t quáº£ Actual Real-time.
        Chá»‰ quÃ©t tin hÃ´m nay.
        """
        url = f"{self.base_url}?day=today"
        logger.info(f"âš¡ scanning Real-time HTML: {url}")
        
        try:
            response = requests.get(url, headers=self.headers, impersonate="chrome120", timeout=30)
            if response.status_code != 200: return

            soup = BeautifulSoup(response.content, "html.parser")
            table = soup.find("table", class_="calendar__table")
            if not table: return
            
            rows = table.find_all("tr", class_="calendar__row")
            current_date_str = ""
            
            for row in rows:
                if "calendar__row--new-day" in row.get("class", []):
                    d = row.find("span", class_="date")
                    if d: current_date_str = d.text.strip()
                
                # HTML has data-event-id, but our DB might use synthetic ID if we used JSON.
                # However, to support the user requirement strictly:
                # "DÃ¹ng BeautifulSoup Ä‘á»ƒ tÃ¬m Ä‘Ãºng dÃ²ng sá»± kiá»‡n (theo data-event-id)" - this strongly suggests the ID is important.
                # Maybe the JSON DOES have IDs? Let's check a sample representation or assume the user knows.
                # If the user is wrong and JSON has no IDs, I will fall back to Name matching.
                # But if I can't check, I will implement robust Name matching. for now.
                
                # Extract Data
                title = row.find("span", class_="calendar__event-title").text.strip()
                currency = row.find("td", class_="calendar__currency").text.strip()
                actual = row.find("td", class_="calendar__actual").text.strip()
                result_time = row.find("td", class_="calendar__time").text.strip()
                
                # Skip if no actual
                if not actual: continue
                
                # Parse Time to construct ID/Match
                # This needs to match the JSON's parsed time.
                # JSON timestamp is UTC ISO.
                # HTML time is NY time usually.
                
                dt_utc = self.parse_datetime(current_date_str, result_time)
                if not dt_utc: continue
                
                timestamp_iso = dt_utc.strftime('%Y-%m-%d %H:%M:%S')
                
                # Reconstruct ID used in Sync
                id_str = f"{timestamp_iso}_{currency}_{title}".replace(" ", "_").replace("/", "").replace(":", "")
                
                # Update DB
                # logic: Find event with this ID (or close match) and update Actual
                # However, `upsert_economic_event` requires full dict.
                # We can write a specific update method or re-use upsert with all fields.
                # Better: `database` should have `update_event_actual(id, actual)`
                
                # Since I can't easily modify database.py in this step (task restricted?), 
                # I will use `upsert` but I need other fields? 
                # Actually `upsert` in sqlite updates if exists. I can try just passing the fields I know?
                # No, upsert replaces/updates. 
                
                # Wait, the user asked to "Refactor ... economic_calendar.py". 
                # I should handle the logic here.
                # If I use `database.upsert_economic_event`, it updates actual if conflict ID.
                # So reuse the ID.
                
                event_dict = {
                    "id": id_str,
                    # We might overwrite Title/Forecast if they differ slightly, but Actual is what matters.
                    # Safety: If row doesn't exist in DB (JSON didn't catch it?), we insert it. Good.
                    "title": title,
                    "currency": currency,
                    "impact": "High", # HTML scraping might need to re-check impact or assume valid
                    "timestamp": timestamp_iso,
                    "forecast": row.find("td", class_="calendar__forecast").text.strip(),
                    "previous": row.find("td", class_="calendar__previous").text.strip(),
                    "actual": actual,
                    "event": title
                }
                
                database.upsert_economic_event(event_dict)
                
        except Exception as e:
            logger.error(f"âŒ Error scanning HTML: {e}")

    def parse_datetime(self, date_str, time_str):
        # ... Reuse existing logic ...
        try:
            if not date_str or not time_str: return None
            clean_date = " ".join(date_str.split()[1:]) if len(date_str.split()) > 1 else date_str
            full_str = f"{clean_date} {datetime.now().year} {time_str}"
            dt_naive = date_parser.parse(full_str)
            ny_tz = tz.gettz('America/New_York')
            dt_ny = dt_naive.replace(tzinfo=ny_tz)
            return dt_ny.astimezone(tz.UTC)
        except: return None

    def process_calendar_alerts(self):
        """
        Main Loop:
        1. Sync Schedule (JSON) periodically (every hour or if empty).
        2. Check for pending events today -> Scan HTML for results.
        3. Send Alerts.
        """
        try:
            # 1. Sync Schedule if needed
            # Simple check: sync every cycle? No, JSON is cached 1h.
            # Just call sync, it handles cache.
            self.sync_schedule_to_db()
            
            # 2. Check for Pending High Impact events happening NOW or RECENTLY
            # We look for events with status 'pre_notified' (sent alert, waiting results)
            # or 'pending' but timestamp has passed.
            
            incomplete = database.get_incomplete_events_today()
            # If there are incomplete events today, scan HTML
            if incomplete:
                self.fetch_realtime_results_html()
            
            # 3. Send Alerts (Pre/Post)
            now_utc = datetime.now(tz.UTC)
            
            # Pre-Alerts
            pre_alerts = database.get_pending_pre_alerts(60)
            for event in pre_alerts:
                ts = date_parser.parse(event['timestamp'])
                if ts.tzinfo is None: ts = ts.replace(tzinfo=tz.UTC)
                diff = (ts - now_utc).total_seconds() / 60
                self.send_pre_alert(event, int(diff))
                database.update_event_status(event['id'], 'pre_notified')
                
            # Post-Alerts
            post_alerts = database.get_pending_post_alerts()
            for event in post_alerts:
                self.send_post_alert(event)
                database.update_event_status(event['id'], 'post_notified')

        except Exception as e:
            logger.error(f"Error in hybrid process: {e}")

    # ... send_pre_alert and send_post_alert methods remain mostly same ...
    def send_pre_alert(self, event, minutes_left):
        try:
             ts = date_parser.parse(event['timestamp'])
             vn_tz = tz.gettz('Asia/Ho_Chi_Minh')
             time_str = ts.astimezone(vn_tz).strftime('%H:%M')
        except: time_str = event['time'] # timestamp might be missing if scraping failed?

        analysis = ai_engine.analyze_pre_economic_data(event)
        
        if analysis:
            exp = html.escape(analysis.get('explanation', ''))
            high = html.escape(analysis.get('scenario_high', ''))
            low = html.escape(analysis.get('scenario_low', ''))
            msg = (f"ğŸ“¢ <b>Báº¢N TIN CHUáº¨N Bá»Š (TrÆ°á»›c {minutes_left}p)</b>\n\n"
                   f"ğŸ”¥ <b>Sáº®P CÃ“ TIN Máº NH ({time_str}): {event['title']}</b>\n"
                   f"âš ï¸ Cáº·p tiá»n: {event['currency']} Pairs\n"
                   f"ğŸ“Š Dá»¯ liá»‡u: Dá»± bÃ¡o {event['forecast']} (Ká»³ trÆ°á»›c {event['previous']})\n\n"
                   f"ğŸ’¡ <b>PhÃ¢n tÃ­ch:</b> {exp}\n\n"
                   f"â†—ï¸ <b>{high}</b>\n"
                   f"â†˜ï¸ <b>{low}</b>\n\n"
                   f"#PreNews")
        else:
            msg = (f"ğŸ“¢ <b>Báº¢N TIN CHUáº¨N Bá»Š (TrÆ°á»›c {minutes_left}p)</b>\n\n"
                   f"ğŸ”¥ <b>Sáº®P CÃ“ TIN Máº NH ({time_str}): {event['title']}</b>\n"
                   f"âš ï¸ Cáº·p tiá»n: {event['currency']} Pairs\n"
                   f"ğŸ“Š Dá»¯ liá»‡u: Dá»± bÃ¡o {event['forecast']} (Ká»³ trÆ°á»›c {event['previous']})\n\n"
                   f"#PreNews")
        telegram_bot.send_message(msg)
        
    def send_post_alert(self, event):
        analysis = ai_engine.analyze_economic_data(event)
        if analysis:
            score = analysis['sentiment_score']
            icon = "ğŸ“ˆ" if score > 0 else "ğŸ“‰" if score < 0 else "â–"
            clean_analysis = html.escape(analysis['impact_analysis'])
            msg = (f"ğŸ“¢ <b>Báº¢N TIN Káº¾T QUáº¢</b>\n\n"
                   f"âš¡ <b>Káº¾T QUáº¢ TIN: {event['title']}</b>\n"
                   f"ğŸ”¢ Actual: <b>{event['actual']}</b> (Dá»± bÃ¡o {event['forecast']}) {icon}\n"
                   f"ğŸ‘‰ <b>ÄÃ¡nh giÃ¡:</b> {score}/10 ({analysis['conclusion']})\n"
                   f"ğŸ“‰ <b>PhÃ¢n tÃ­ch:</b> {clean_analysis}\n\n"
                   f"#EconomicResult")
        else:
            msg = (f"ğŸ“¢ <b>Báº¢N TIN Káº¾T QUáº¢</b>\n\n"
                   f"âš¡ <b>Káº¾T QUáº¢ TIN: {event['title']}</b>\n"
                   f"ğŸ”¢ Actual: <b>{event['actual']}</b> (Dá»± bÃ¡o {event['forecast']})\n"
                   f"#EconomicResult")
        telegram_bot.send_message(msg)
