import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dateutil import parser
import re
import time
import json
from urllib.parse import urljoin
from typing import List, Dict, Optional, Any
from app.core import config
from app.core import database # Updated import

logger = config.logger
KEYWORDS = {
    "DIRECT": config.KEYWORDS_DIRECT,
    "CORRELATION": config.KEYWORDS_CORRELATION
}
HEADERS = config.HEADERS


def clean_html(raw_html: str) -> str:
    cleanr = re.compile('<.*?>')
    return re.sub(cleanr, '', raw_html).strip()

def check_keywords(text: str) -> List[str]:
    found_keywords = []
    text_lower = text.lower()
    all_keywords = KEYWORDS["DIRECT"] + KEYWORDS["CORRELATION"]
    for kw in all_keywords:
        pattern = r"\b" + re.escape(kw.lower()) + r"\b"
        if re.search(pattern, text_lower):
            found_keywords.append(kw)
    return list(set(found_keywords)) # Lo·∫°i b·ªè keyword tr√πng l·∫∑p

def get_full_content(url: str, selector: str = None) -> str:
    """L·∫•y n·ªôi dung b√†i vi·∫øt full, h·ªó tr·ª£ selector ƒë·ªông"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code != 200: 
            return "L·ªói truy c·∫≠p (Ch·∫∑n Bot)"
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        paragraphs = []
        # 1. D√πng Selector n·∫øu c√≥ c·∫•u h√¨nh
        if selector:
            paragraphs = soup.select(selector)
        
        # 2. Fallback: T·ª± ƒë·ªông ƒëo√°n n·∫øu ch∆∞a t√¨m th·∫•y
        if not paragraphs:
            # CMS Detection Fallback (Legacy)
            if "cnn.com" in url:
                paragraphs = soup.select("div.article__content p")
            else:
                paragraphs = soup.find_all('p')
            
        full_text = "\\n\\n".join([p.get_text().strip() for p in paragraphs])
        return full_text if len(full_text) > 200 else "N·ªôi dung qu√° ng·∫Øn/b·ªã ·∫©n."
    except Exception as e:
        return f"L·ªói c√†o d·ªØ li·ªáu: {e}"

def get_rss_feed_data(url: str):
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        return feedparser.parse(response.content)
    except Exception as e:
        logger.error(f"‚ö†Ô∏è RSS {url} l·ªói: {e}")
        return None

def scrape_website_fallback(source_config: Dict) -> List[Dict]:
    """C√†o tr·ª±c ti·∫øp website n·∫øu RSS l·ªói (Dynamic URL)"""
    url = source_config.get("web")
    source_name = source_config.get("name")
    
    if not url:
        return []

    logger.info(f"üîÑ ƒêang k√≠ch ho·∫°t Web Scraping cho {source_name} ({url})...")
    entries = []
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Heuristic: T√¨m t·∫•t c·∫£ th·∫ª A c√≥ text ƒë·ªß d√†i
        links = soup.find_all('a', href=True)
        seen_titles = set()
        
        for a in links:
            title = a.get_text().strip()
            href = a['href']
            
            # L·ªçc r√°c
            if len(title) < 20: continue
            if "javascript:" in href or "mailto:" in href: continue
             
            # Chu·∫©n h√≥a URL dynamic b·∫±ng urljoin
            full_link = urljoin(url, href)
            
            # Ch·ªâ l·∫•y tin c√≥ keyword
            if not check_keywords(title):
                continue
                
            if title in seen_titles: continue
            seen_titles.add(title)

            entries.append({
                "title": title,
                "link": full_link,
                "summary": "",
                "published": datetime.now(timezone.utc).isoformat()
            })
            
        logger.info(f"‚úÖ Web Scraping t√¨m th·∫•y {len(entries)} b√†i vi·∫øt ti·ªÅm nƒÉng.")
        return entries
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói Web Scraping {source_name}: {e}")
        return []

def get_gold_news(lookback_minutes: Optional[int] = None) -> List[Dict[str, Any]]:
    """
    Qu√©t tin t·ª©c t·ª´ RSS v√† Web Fallback.
    Args:
        lookback_minutes: N·∫øu c√≥, ch·ªâ l·∫•y tin trong kho·∫£ng th·ªùi gian n√†y.
    Returns:
        List[Dict]: Danh s√°ch c√°c b√†i vi·∫øt M·ªöI v·ª´a ƒë∆∞·ª£c th√™m v√†o DB.
    """
    logger.info(">>> KH·ªûI T·∫†O DATABASE...")
    database.init_db() 
    
    logger.info(f">>> ƒêANG QU√âT TIN T·ª®C... (Lookback: {lookback_minutes if lookback_minutes else '24h'})")
    now_utc = datetime.now(timezone.utc)
    
    # X√°c ƒë·ªãnh gi·ªõi h·∫°n th·ªùi gian
    if lookback_minutes:
        time_limit = now_utc - timedelta(minutes=lookback_minutes)
    else:
        time_limit = now_utc - timedelta(hours=24) 
    
    new_articles_added = []
    new_articles_count = 0

    for source in config.NEWS_SOURCES:
        entries = []
        is_fallback = False
        source_name = source.get("name", "Unknown")
        rss_url = source.get("rss")
        selector = source.get("selector")
        
        # 1. Th·ª≠ RSS tr∆∞·ªõc
        try:
            feed = get_rss_feed_data(rss_url)
            if feed and feed.entries:
                entries = feed.entries
                logger.info(f"-> RSS {source_name}: Qu√©t {len(entries)} b√†i...")
            else:
                raise Exception("RSS Empty/Fail")
        except:
            # 2. RSS L·ªói -> Th·ª≠ Web Scraping
            logger.warning(f"‚ö†Ô∏è RSS {source_name} th·∫•t b·∫°i. Chuy·ªÉn sang Web Scraping...")
            entries = scrape_website_fallback(source)
            is_fallback = True
        
        if not entries:
            continue

        # 3. X·ª≠ l√Ω danh s√°ch b√†i vi·∫øt (t·ª´ RSS ho·∫∑c Web)
        for entry in entries:
            # Chu·∫©n h√≥a field (feedparser d√πng object, scraping d√πng dict)
            if isinstance(entry, dict):
                link = entry.get("link", "")
                title = entry.get("title", "")
                summary = entry.get("summary", "")
                pub_str = entry.get("published", "")
            else:
                link = getattr(entry, "link", "")
                title = getattr(entry, "title", "")
                summary = clean_html(getattr(entry, "summary", ""))
                pub_str = getattr(entry, "published", getattr(entry, "updated", ""))

            if not link or not title: continue
            
            # KI·ªÇM TRA TR√ôNG
            if database.check_article_exists(link):
                continue

            # X·ª≠ l√Ω th·ªùi gian (Ch·ªâ check k·ªπ v·ªõi RSS, Web scraping l·∫•y tin m·ªõi nh·∫•t)
            if not is_fallback:
                try:
                    pub_date = parser.parse(pub_str)
                    if pub_date.tzinfo is None: pub_date = pub_date.replace(tzinfo=timezone.utc)
                    if pub_date < time_limit: continue
                except: continue
            else:
                # V·ªõi Web Fallback, m·∫∑c ƒë·ªãnh tin l·∫•y v·ªÅ l√† "m·ªõi" n·∫øu ch∆∞a c√≥ trong DB
                # nh∆∞ng ƒë·ªÉ an to√†n, g√°n time hi·ªán t·∫°i
                pub_date = now_utc

            # Check Keyword (Double check cho ch·∫Øc ch·∫Øn)
            matched_kws = check_keywords(title + " " + summary)
            
            if matched_kws:
                logger.info(f"   [+] Tin m·ªõi ({'WEB' if is_fallback else 'RSS'}): {title[:50]}...")
                
                # Truy·ªÅn selector v√†o h√†m get_full_content
                full_content = get_full_content(link, selector=selector)
                
                news_item = {
                    "id": link,
                    "source": source_name,
                    "published_at": pub_date.isoformat(),
                    "title": title,
                    "keywords": matched_kws,
                    "url": link,
                    "content": full_content
                }
                
                if database.save_to_db(news_item):
                    new_articles_count += 1
                    new_articles_added.append(news_item)
                
                time.sleep(1) # Delay nh·∫π

    logger.info("="*60)
    logger.info(f"‚úÖ HO√ÄN T·∫§T! ƒê√£ th√™m {new_articles_count} b√†i vi·∫øt m·ªõi v√†o Database.")
    logger.info("="*60)
    return new_articles_added
