import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dateutil import parser
import re
import time
import json
import sqlite3
from contextlib import contextmanager
from typing import List, Dict, Optional, Any
import config # Import config module

# --- C·∫§U H√åNH ---
# S·ª≠ d·ª•ng bi·∫øn t·ª´ config.py
DB_NAME = config.DB_NAME
KEYWORDS = {
    "DIRECT": config.KEYWORDS_DIRECT,
    "CORRELATION": config.KEYWORDS_CORRELATION
}
HEADERS = config.HEADERS
logger = config.logger

@contextmanager
def get_db_connection():
    """Context manager ƒë·ªÉ qu·∫£n l√Ω k·∫øt n·ªëi DB an to√†n"""
    conn = None
    try:
        conn = sqlite3.connect(DB_NAME)
        conn.row_factory = sqlite3.Row # Tr·∫£ v·ªÅ Row object thay v√¨ tuple
        yield conn
    except sqlite3.Error as e:
        logger.error(f"L·ªói k·∫øt n·ªëi CSDL: {e}")
        raise e
    finally:
        if conn:
            conn.close()

# --- PH·∫¶N DATABASE (M·ªöI) ---
def init_db() -> None:
    """Kh·ªüi t·∫°o b·∫£ng n·∫øu ch∆∞a c√≥"""
    try:
        with get_db_connection() as conn:
            c = conn.cursor()
            # T·∫°o b·∫£ng articles
            c.execute('''
                CREATE TABLE IF NOT EXISTS articles (
                    id TEXT PRIMARY KEY,       -- Link b√†i vi·∫øt l√† kh√≥a ch√≠nh
                    source TEXT,
                    title TEXT,
                    published TEXT,
                    content TEXT,              -- N·ªôi dung full
                    keywords TEXT,             -- L∆∞u list keyword d·∫°ng string
                    status TEXT DEFAULT 'NEW', -- NEW: Ch∆∞a AI x·ª≠ l√Ω, PROCESSED: ƒê√£ xong
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            # T·∫°o b·∫£ng reports
            c.execute('''
                CREATE TABLE IF NOT EXISTS reports (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    report_content TEXT,    -- N·ªôi dung b√†i vi·∫øt final
                    sentiment_score REAL,   -- ƒêi·ªÉm s·ªë (-10 ƒë·∫øn 10)
                    trend TEXT,             -- Bullish/Bearish/Neutral
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            conn.commit()
    except Exception as e:
        logger.error(f"L·ªói kh·ªüi t·∫°o DB: {e}")

def check_article_exists(link: str) -> bool:
    """Ki·ªÉm tra link ƒë√£ c√≥ trong DB ch∆∞a"""
    try:
        with get_db_connection() as conn:
            c = conn.cursor()
            c.execute("SELECT 1 FROM articles WHERE id = ?", (link,))
            return c.fetchone() is not None
    except Exception:
        return False

def save_to_db(item: Dict[str, Any]) -> bool:
    """L∆∞u 1 b√†i b√°o v√†o DB"""
    try:
        with get_db_connection() as conn:
            c = conn.cursor()
            # Chuy·ªÉn list keywords th√†nh chu·ªói JSON ƒë·ªÉ l∆∞u v√†o c·ªôt TEXT
            keywords_str = json.dumps(item["keywords"], ensure_ascii=False)
            
            c.execute('''
                INSERT OR IGNORE INTO articles (id, source, title, published, content, keywords, status)
                VALUES (?, ?, ?, ?, ?, ?, 'NEW')
            ''', (
                item["id"],
                item["source"],
                item["title"],
                item["published_at"],
                item["content"],
                keywords_str
            ))
            conn.commit()
            return True
    except Exception as e:
        logger.error(f"L·ªói l∆∞u DB b√†i vi·∫øt {item.get('id')}: {e}")
        return False

# --- C√ÅC H√ÄM CRAWL/PARSE ---
def clean_html(raw_html: str) -> str:
    cleanr = re.compile('<.*?>')
    return re.sub(cleanr, '', raw_html).strip()

def check_keywords(text: str) -> List[str]:
    found_keywords = []
    text_lower = text.lower()
    all_keywords = KEYWORDS["DIRECT"] + KEYWORDS["CORRELATION"]
    for kw in all_keywords:
        pattern = r"\b" + re.escape(kw.lower()) + r"\b"
        if re.search(pattern, text_lower):
            found_keywords.append(kw)
    return list(set(found_keywords)) # Lo·∫°i b·ªè keyword tr√πng l·∫∑p

def get_full_content(url: str) -> str:
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code != 200: 
            return "L·ªói truy c·∫≠p (Ch·∫∑n Bot)"
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        if "kitco.com" in url:
            paragraphs = soup.select("div.article-body p")
        elif "investing.com" in url:
            paragraphs = soup.select("div.WYSIWYG p")
        else:
            paragraphs = soup.find_all('p')
            
        full_text = "\\n\\n".join([p.get_text().strip() for p in paragraphs])
        return full_text if len(full_text) > 200 else "N·ªôi dung qu√° ng·∫Øn/b·ªã ·∫©n."
    except Exception as e:
        return f"L·ªói c√†o d·ªØ li·ªáu: {e}"

def get_rss_feed_data(url: str):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        return feedparser.parse(response.content)
    except:
        return None

# --- H√ÄM CH√çNH ---
def get_gold_news():
    logger.info(">>> KH·ªûI T·∫†O DATABASE...")
    init_db() # 1. T·∫°o b·∫£ng n·∫øu ch∆∞a c√≥
    
    logger.info(">>> ƒêANG QU√âT TIN T·ª®C...")
    now_utc = datetime.now(timezone.utc)
    time_limit = now_utc - timedelta(hours=24) 
    
    new_articles_count = 0

    for source in config.RSS_SOURCES:
        try:
            feed = get_rss_feed_data(source["url"])
            if not feed or not feed.entries:
                logger.warning(f"-> {source['name']}: Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu.")
                continue

            logger.info(f"-> {source['name']}: Qu√©t {len(feed.entries)} b√†i...")
            
            for entry in feed.entries:
                link = entry.get("link", "")
                
                # 2. KI·ªÇM TRA T·ªíN T·∫†I TR∆Ø·ªöC
                if check_article_exists(link):
                    continue

                # X·ª≠ l√Ω ng√†y th√°ng
                published = entry.get("published", entry.get("updated", ""))
                if not published: continue
                try:
                    pub_date = parser.parse(published)
                    if pub_date.tzinfo is None: pub_date = pub_date.replace(tzinfo=timezone.utc)
                    if pub_date < time_limit: continue
                except: continue

                title = entry.get("title", "")
                summary = clean_html(entry.get("summary", ""))
                
                # Check Keyword
                matched_kws = check_keywords(title + " " + summary)
                
                if matched_kws:
                    logger.info(f"   [+] Tin m·ªõi: {title[:50]}...")
                    
                    full_content = get_full_content(link)
                    
                    news_item = {
                        "id": link,
                        "source": source["name"],
                        "published_at": pub_date.isoformat(),
                        "title": title,
                        "keywords": matched_kws,
                        "url": link,
                        "content": full_content
                    }
                    
                    # 3. L∆ØU V√ÄO DB
                    save_to_db(news_item)
                    new_articles_count += 1
                    time.sleep(1) # Delay nh·∫π
            
        except Exception as e:
            logger.error(f"L·ªói ngu·ªìn {source['name']}: {e}")

    logger.info("="*60)
    logger.info(f"‚úÖ HO√ÄN T·∫§T! ƒê√£ th√™m {new_articles_count} b√†i vi·∫øt m·ªõi v√†o Database.")
    logger.info("="*60)

if __name__ == "__main__":
    get_gold_news()

# --- H√ÄM PUBLIC CHO NGHI·ªÜP V·ª§ KH√ÅC ---

def get_unprocessed_articles() -> List[Dict[str, Any]]:
    """L·∫•y t·∫•t c·∫£ b√†i vi·∫øt c√≥ status = 'NEW'"""
    try:
        with get_db_connection() as conn:
            c = conn.cursor()
            c.execute("SELECT id, source, title, content FROM articles WHERE status = 'NEW'")
            rows = c.fetchall()
            return [dict(row) for row in rows]
    except Exception as e:
        logger.error(f"L·ªói l·∫•y b√†i vi·∫øt ch∆∞a x·ª≠ l√Ω: {e}")
        return []

def mark_articles_processed(ids: List[str]) -> None:
    """Chuy·ªÉn status sang PROCESSED sau khi AI ph√¢n t√≠ch xong"""
    if not ids: return
    try:
        with get_db_connection() as conn:
            c = conn.cursor()
            placeholders = ','.join(['?'] * len(ids))
            sql = f"UPDATE articles SET status = 'PROCESSED' WHERE id IN ({placeholders})"
            c.execute(sql, ids)
            conn.commit()
    except Exception as e:
        logger.error(f"L·ªói c·∫≠p nh·∫≠t tr·∫°ng th√°i b√†i vi·∫øt: {e}")

def save_report(content: str, score: float, trend: str) -> None:
    try:
        with get_db_connection() as conn:
            c = conn.cursor()
            c.execute("INSERT INTO reports (report_content, sentiment_score, trend) VALUES (?, ?, ?)", 
                      (content, score, trend))
            conn.commit()
            logger.info("üíæ ƒê√£ l∆∞u b√°o c√°o ph√¢n t√≠ch v√†o Database.")
    except Exception as e:
        logger.error(f"L·ªói l∆∞u b√°o c√°o: {e}")

def get_latest_report() -> Optional[Dict[str, Any]]:
    """L·∫•y b√°o c√°o ph√¢n t√≠ch g·∫ßn nh·∫•t ƒë·ªÉ l√†m context"""
    try:
        with get_db_connection() as conn:
            c = conn.cursor()
            c.execute("SELECT sentiment_score, trend, created_at FROM reports ORDER BY id DESC LIMIT 1")
            row = c.fetchone()
            return dict(row) if row else None
    except Exception as e:
        logger.error(f"L·ªói l·∫•y b√°o c√°o m·ªõi nh·∫•t: {e}")
        return None